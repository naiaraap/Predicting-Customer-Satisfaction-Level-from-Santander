{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Customer Satisfaction Level from Santander "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is on the kaggle platform (link in cell below). The dataset is anonymized and consists of a large number of numeric variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/santander-customer-satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries and frameworks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing train data\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing test data\n",
    "df_data_test = pd.read_csv(\"data/test.csv\")\n",
    "df_result_test = pd.read_csv(\"data/sample_submission.csv\")\n",
    "df_test = df_data_test.merge(df_result_test, on = 'ID')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving columns names and modifing it\n",
    "original_col_names = df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting new columns names for to facilitate data manipulation.\n",
    "new_col_names = [\"ID\"]\n",
    "new_col_names = new_col_names + ([\"var\" + str(i) for i in range(1,370)])\n",
    "new_col_names = new_col_names + [\"TARGET\"]\n",
    "\n",
    "df_train.columns = new_col_names\n",
    "df_test.columns = new_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the number of characteristics is very large I applied the Principal Component Analysis algorithm to reduce the size of the dataset and facilitate the analyzes and transformations that precede the creation of the predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "print(pd.isna(df_train).any().any())\n",
    "print(pd.isna(df_test).any().any())\n",
    "print(pd.isnull(df_train).any().any())\n",
    "print(pd.isnull(df_test).any().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saved dataset before applying transformations. Then I dropped \"ID\" variable and converting all independent variables to float. So I got the features with a proportion of non-zero values greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving dataset\n",
    "df_train.to_csv(\"data/df_train.csv\", index=False)\n",
    "df_test.to_csv(\"data/df_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing duplicated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I eliminated the duplicate columns applying drop_duplicates function to pandas in the transposed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicate_train = df_train.T.drop_duplicates(keep='first').T\n",
    "df_no_duplicate_test = df_test[list(df_no_duplicate_train.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying more filters to features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before filtering features, I normalized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "df_normalized_train = pd.DataFrame(normalizer.fit_transform(df_no_duplicate_train.drop(\"ID\", axis=1)),\n",
    "                                  columns = df_no_duplicate_train.drop(\"ID\", axis=1).columns)\n",
    "\n",
    "normalizer = Normalizer()\n",
    "df_normalized_test = pd.DataFrame(normalizer.fit_transform(df_no_duplicate_test.drop(\"ID\", axis=1)),\n",
    "                                  columns = df_no_duplicate_test.drop(\"ID\", axis=1).columns)\n",
    "\n",
    "df_normalized_train.to_csv(\"data/df_normalized_train.csv\", index=False)\n",
    "df_normalized_test.to_csv(\"data/df_normalized_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing constant and Quasi-Constant Features Using Variance Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed constant and quasi-constant variables from my dataset using variance threshhold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing quasi-constant columns\n",
    "constant_filter = VarianceThreshold(threshold=0.0001)\n",
    "constant_filter.fit(df_normalized_train.drop(\"TARGET\", axis=1))\n",
    "\n",
    "filtered_columns = [column for column in df_normalized_train.drop(\"TARGET\", axis=1).columns\n",
    "                    if column not in df_normalized_train.drop(\"TARGET\", axis=1) \\\n",
    "                    .columns[constant_filter.get_support()]]\n",
    "\n",
    "\n",
    "df_filtered_train = df_normalized_train.drop(filtered_columns, axis=1)\n",
    "df_filtered_test = df_normalized_test.drop(filtered_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 42)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Highly Correlated Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I removed the highly correlated independent features. I created a correlation matrix for filter low-correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_filtered_train.corr()\n",
    "correlated_features = set()\n",
    "\n",
    "[correlated_features.add(rowname) for rowname in correlation_matrix.columns for colname in correlation_matrix.columns if \\\n",
    "correlation_matrix.loc[rowname][colname] > 0.8 and rowname != colname]\n",
    "\n",
    "df_filter_norm_train = df_filtered_train.drop(labels=correlated_features, axis=1)\n",
    "df_filter_norm_test = df_filtered_test.drop(labels=correlated_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter_norm_train.to_csv(\"data/df_filter_norm_train.csv\")\n",
    "df_filter_norm_test.to_csv(\"data/df_filter_norm_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced_train = df_train[list(df_filter_norm_train.columns)]\n",
    "df_reduced_test = df_test[list(df_filter_norm_train.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced_train.to_csv()\n",
    "df_reduced_test.to_csv()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
